"""
Indexing Monitor

This module provides comprehensive monitoring and status tracking
for the vector indexing system with metrics, alerting, and health checks.
"""

import asyncio
import logging
import time
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
from collections import deque, defaultdict
import json

from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)


class HealthStatus(str, Enum):
    """Health status levels."""
    
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"
    CRITICAL = "critical"


class AlertSeverity(str, Enum):
    """Alert severity levels."""
    
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


class MetricType(str, Enum):
    """Types of metrics to track."""
    
    COUNTER = "counter"
    GAUGE = "gauge"
    HISTOGRAM = "histogram"
    TIMER = "timer"


@dataclass
class MetricValue:
    """A metric value with timestamp."""
    
    value: float
    timestamp: datetime = field(default_factory=datetime.utcnow)
    labels: Dict[str, str] = field(default_factory=dict)


@dataclass
class Alert:
    """An alert generated by the monitoring system."""
    
    id: str
    severity: AlertSeverity
    title: str
    message: str
    metric_name: str
    threshold_value: float
    current_value: float
    timestamp: datetime = field(default_factory=datetime.utcnow)
    resolved: bool = False
    resolved_at: Optional[datetime] = None


class MonitoringConfiguration(BaseModel):
    """Configuration for the monitoring system."""
    
    # Metric collection
    collection_interval: float = Field(default=30.0, description="Metric collection interval in seconds")
    retention_period: int = Field(default=3600, description="Metric retention period in seconds")
    max_metrics_per_type: int = Field(default=1000, description="Maximum metrics to retain per type")
    
    # Health check thresholds
    error_rate_threshold: float = Field(default=0.1, description="Error rate threshold for health status")
    latency_threshold: float = Field(default=10.0, description="Latency threshold in seconds")
    queue_size_threshold: int = Field(default=1000, description="Queue size threshold")
    
    # Alerting
    enable_alerting: bool = Field(default=True, description="Enable alerting")
    alert_cooldown: float = Field(default=300.0, description="Alert cooldown period in seconds")
    max_alerts: int = Field(default=100, description="Maximum alerts to retain")
    
    # Performance monitoring
    track_performance: bool = Field(default=True, description="Enable performance tracking")
    performance_window: int = Field(default=300, description="Performance monitoring window in seconds")


class IndexingMetrics(BaseModel):
    """Metrics for indexing operations."""
    
    # Throughput metrics
    total_requests: int = Field(default=0, description="Total indexing requests")
    completed_requests: int = Field(default=0, description="Completed requests")
    failed_requests: int = Field(default=0, description="Failed requests")
    
    # Performance metrics
    average_latency: float = Field(default=0.0, description="Average request latency")
    requests_per_second: float = Field(default=0.0, description="Requests per second")
    chunks_per_second: float = Field(default=0.0, description="Chunks processed per second")
    
    # Queue metrics
    queue_size: int = Field(default=0, description="Current queue size")
    processing_items: int = Field(default=0, description="Items currently processing")
    
    # Error metrics
    error_rate: float = Field(default=0.0, description="Error rate percentage")
    retry_rate: float = Field(default=0.0, description="Retry rate percentage")
    dlq_size: int = Field(default=0, description="Dead letter queue size")
    
    # Resource metrics
    memory_usage: float = Field(default=0.0, description="Memory usage percentage")
    cpu_usage: float = Field(default=0.0, description="CPU usage percentage")
    
    # Timestamp
    timestamp: datetime = Field(default_factory=datetime.utcnow, description="Metrics timestamp")


class IndexingMonitor:
    """Monitor for vector indexing operations."""
    
    def __init__(self, config: MonitoringConfiguration = None):
        self.config = config or MonitoringConfiguration()
        
        # Metrics storage
        self._metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=self.config.max_metrics_per_type))
        self._current_metrics = IndexingMetrics()
        
        # Alerting
        self._alerts: deque = deque(maxlen=self.config.max_alerts)
        self._alert_cooldowns: Dict[str, datetime] = {}
        self._alert_handlers: List[Callable] = []
        
        # Health tracking
        self._health_status = HealthStatus.HEALTHY
        self._health_checks: Dict[str, Callable] = {}
        
        # Performance tracking
        self._performance_window: deque = deque(maxlen=int(self.config.performance_window / self.config.collection_interval))
        
        # Monitoring state
        self._monitoring_task: Optional[asyncio.Task] = None
        self._running = False
        
        # Register default health checks
        self._register_default_health_checks()
        
        logger.info(f"Indexing monitor initialized with config: {self.config}")
    
    def _register_default_health_checks(self):
        """Register default health check functions."""
        
        async def check_error_rate():
            if self._current_metrics.error_rate > self.config.error_rate_threshold:
                return HealthStatus.DEGRADED, f"High error rate: {self._current_metrics.error_rate:.2%}"
            return HealthStatus.HEALTHY, "Error rate normal"
        
        async def check_latency():
            if self._current_metrics.average_latency > self.config.latency_threshold:
                return HealthStatus.DEGRADED, f"High latency: {self._current_metrics.average_latency:.2f}s"
            return HealthStatus.HEALTHY, "Latency normal"
        
        async def check_queue_size():
            if self._current_metrics.queue_size > self.config.queue_size_threshold:
                return HealthStatus.DEGRADED, f"Large queue: {self._current_metrics.queue_size} items"
            return HealthStatus.HEALTHY, "Queue size normal"
        
        self._health_checks["error_rate"] = check_error_rate
        self._health_checks["latency"] = check_latency
        self._health_checks["queue_size"] = check_queue_size
    
    async def start_monitoring(self):
        """Start the monitoring system."""
        if self._running:
            return
        
        self._running = True
        self._monitoring_task = asyncio.create_task(self._monitoring_loop())
        logger.info("Indexing monitoring started")
    
    async def stop_monitoring(self):
        """Stop the monitoring system."""
        self._running = False
        
        if self._monitoring_task:
            self._monitoring_task.cancel()
            try:
                await self._monitoring_task
            except asyncio.CancelledError:
                pass
        
        logger.info("Indexing monitoring stopped")
    
    async def _monitoring_loop(self):
        """Main monitoring loop."""
        while self._running:
            try:
                # Collect metrics
                await self._collect_metrics()
                
                # Perform health checks
                await self._perform_health_checks()
                
                # Check for alerts
                await self._check_alerts()
                
                # Store metrics
                await self._store_metrics()
                
                # Wait for next collection interval
                await asyncio.sleep(self.config.collection_interval)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Error in monitoring loop: {e}")
                await asyncio.sleep(self.config.collection_interval)
    
    async def _collect_metrics(self):
        """Collect current metrics from the indexing system."""
        # This would be implemented to collect actual metrics from the indexing pipeline
        # For now, we'll use placeholder values
        
        # Update timestamp
        self._current_metrics.timestamp = datetime.utcnow()
        
        # Calculate derived metrics
        if self._current_metrics.total_requests > 0:
            self._current_metrics.error_rate = self._current_metrics.failed_requests / self._current_metrics.total_requests
        
        # Add to performance window
        self._performance_window.append(self._current_metrics.copy())
    
    async def _perform_health_checks(self):
        """Perform all registered health checks."""
        overall_status = HealthStatus.HEALTHY
        health_messages = []
        
        for check_name, check_func in self._health_checks.items():
            try:
                status, message = await check_func()
                health_messages.append(f"{check_name}: {message}")
                
                # Determine overall status (worst case)
                if status == HealthStatus.CRITICAL:
                    overall_status = HealthStatus.CRITICAL
                elif status == HealthStatus.UNHEALTHY and overall_status != HealthStatus.CRITICAL:
                    overall_status = HealthStatus.UNHEALTHY
                elif status == HealthStatus.DEGRADED and overall_status == HealthStatus.HEALTHY:
                    overall_status = HealthStatus.DEGRADED
                    
            except Exception as e:
                logger.error(f"Health check {check_name} failed: {e}")
                health_messages.append(f"{check_name}: check failed")
                if overall_status == HealthStatus.HEALTHY:
                    overall_status = HealthStatus.DEGRADED
        
        # Update health status
        if self._health_status != overall_status:
            logger.info(f"Health status changed from {self._health_status.value} to {overall_status.value}")
            self._health_status = overall_status
    
    async def _check_alerts(self):
        """Check for alert conditions."""
        if not self.config.enable_alerting:
            return
        
        current_time = datetime.utcnow()
        
        # Check error rate alert
        if self._current_metrics.error_rate > self.config.error_rate_threshold:
            await self._trigger_alert(
                "high_error_rate",
                AlertSeverity.WARNING,
                "High Error Rate",
                f"Error rate {self._current_metrics.error_rate:.2%} exceeds threshold {self.config.error_rate_threshold:.2%}",
                "error_rate",
                self.config.error_rate_threshold,
                self._current_metrics.error_rate
            )
        
        # Check latency alert
        if self._current_metrics.average_latency > self.config.latency_threshold:
            await self._trigger_alert(
                "high_latency",
                AlertSeverity.WARNING,
                "High Latency",
                f"Average latency {self._current_metrics.average_latency:.2f}s exceeds threshold {self.config.latency_threshold}s",
                "average_latency",
                self.config.latency_threshold,
                self._current_metrics.average_latency
            )
        
        # Check queue size alert
        if self._current_metrics.queue_size > self.config.queue_size_threshold:
            await self._trigger_alert(
                "large_queue",
                AlertSeverity.WARNING,
                "Large Queue Size",
                f"Queue size {self._current_metrics.queue_size} exceeds threshold {self.config.queue_size_threshold}",
                "queue_size",
                self.config.queue_size_threshold,
                self._current_metrics.queue_size
            )
    
    async def _trigger_alert(
        self,
        alert_id: str,
        severity: AlertSeverity,
        title: str,
        message: str,
        metric_name: str,
        threshold_value: float,
        current_value: float
    ):
        """Trigger an alert if not in cooldown."""
        current_time = datetime.utcnow()
        
        # Check cooldown
        if alert_id in self._alert_cooldowns:
            time_since_last = (current_time - self._alert_cooldowns[alert_id]).total_seconds()
            if time_since_last < self.config.alert_cooldown:
                return
        
        # Create alert
        alert = Alert(
            id=alert_id,
            severity=severity,
            title=title,
            message=message,
            metric_name=metric_name,
            threshold_value=threshold_value,
            current_value=current_value
        )
        
        # Store alert
        self._alerts.append(alert)
        self._alert_cooldowns[alert_id] = current_time
        
        # Notify alert handlers
        for handler in self._alert_handlers:
            try:
                await handler(alert)
            except Exception as e:
                logger.error(f"Alert handler failed: {e}")
        
        logger.warning(f"Alert triggered: {title} - {message}")
    
    async def _store_metrics(self):
        """Store current metrics in time series."""
        timestamp = datetime.utcnow()
        
        # Store individual metrics
        metrics_dict = self._current_metrics.dict()
        for metric_name, value in metrics_dict.items():
            if metric_name != "timestamp" and isinstance(value, (int, float)):
                metric_value = MetricValue(value=value, timestamp=timestamp)
                self._metrics[metric_name].append(metric_value)
    
    def update_metrics(self, **kwargs):
        """Update current metrics."""
        for key, value in kwargs.items():
            if hasattr(self._current_metrics, key):
                setattr(self._current_metrics, key, value)
    
    def increment_counter(self, metric_name: str, value: float = 1.0, labels: Dict[str, str] = None):
        """Increment a counter metric."""
        current_value = getattr(self._current_metrics, metric_name, 0)
        setattr(self._current_metrics, metric_name, current_value + value)
    
    def set_gauge(self, metric_name: str, value: float, labels: Dict[str, str] = None):
        """Set a gauge metric value."""
        setattr(self._current_metrics, metric_name, value)
    
    def get_current_metrics(self) -> IndexingMetrics:
        """Get current metrics snapshot."""
        return self._current_metrics.copy()
    
    def get_metric_history(self, metric_name: str, duration: timedelta = None) -> List[MetricValue]:
        """Get metric history for a specific metric."""
        if metric_name not in self._metrics:
            return []
        
        metrics = list(self._metrics[metric_name])
        
        if duration:
            cutoff_time = datetime.utcnow() - duration
            metrics = [m for m in metrics if m.timestamp >= cutoff_time]
        
        return metrics
    
    def get_health_status(self) -> Dict[str, Any]:
        """Get current health status."""
        return {
            "status": self._health_status.value,
            "timestamp": datetime.utcnow().isoformat(),
            "metrics": self._current_metrics.dict()
        }
    
    def get_alerts(self, severity: AlertSeverity = None, resolved: bool = None) -> List[Alert]:
        """Get alerts with optional filtering."""
        alerts = list(self._alerts)
        
        if severity:
            alerts = [a for a in alerts if a.severity == severity]
        
        if resolved is not None:
            alerts = [a for a in alerts if a.resolved == resolved]
        
        return alerts
    
    async def resolve_alert(self, alert_id: str) -> bool:
        """Resolve an alert."""
        for alert in self._alerts:
            if alert.id == alert_id and not alert.resolved:
                alert.resolved = True
                alert.resolved_at = datetime.utcnow()
                logger.info(f"Alert resolved: {alert.title}")
                return True
        return False
    
    def add_alert_handler(self, handler: Callable[[Alert], None]):
        """Add an alert handler function."""
        self._alert_handlers.append(handler)
    
    def add_health_check(self, name: str, check_func: Callable[[], tuple]):
        """Add a custom health check."""
        self._health_checks[name] = check_func
    
    def get_performance_summary(self, duration: timedelta = None) -> Dict[str, Any]:
        """Get performance summary over a time period."""
        if not duration:
            duration = timedelta(seconds=self.config.performance_window)
        
        cutoff_time = datetime.utcnow() - duration
        recent_metrics = [m for m in self._performance_window if m.timestamp >= cutoff_time]
        
        if not recent_metrics:
            return {"status": "no_data", "duration": duration.total_seconds()}
        
        # Calculate aggregates
        total_requests = sum(m.total_requests for m in recent_metrics)
        total_failed = sum(m.failed_requests for m in recent_metrics)
        avg_latency = sum(m.average_latency for m in recent_metrics) / len(recent_metrics)
        avg_throughput = sum(m.requests_per_second for m in recent_metrics) / len(recent_metrics)
        
        return {
            "status": "ok",
            "duration": duration.total_seconds(),
            "sample_count": len(recent_metrics),
            "total_requests": total_requests,
            "total_failed": total_failed,
            "error_rate": total_failed / total_requests if total_requests > 0 else 0.0,
            "average_latency": avg_latency,
            "average_throughput": avg_throughput,
            "health_status": self._health_status.value
        }
    
    async def export_metrics(self, format: str = "json") -> str:
        """Export metrics in specified format."""
        if format == "json":
            # Get current metrics and convert datetime objects
            current_metrics = self._current_metrics.dict()
            if 'timestamp' in current_metrics and current_metrics['timestamp']:
                current_metrics['timestamp'] = current_metrics['timestamp'].isoformat()

            export_data = {
                "timestamp": datetime.utcnow().isoformat(),
                "current_metrics": current_metrics,
                "health_status": self._health_status.value,
                "alerts": [
                    {
                        "id": a.id,
                        "severity": a.severity.value,
                        "title": a.title,
                        "message": a.message,
                        "timestamp": a.timestamp.isoformat(),
                        "resolved": a.resolved,
                        "resolved_at": a.resolved_at.isoformat() if a.resolved_at else None
                    }
                    for a in self._alerts
                ],
                "performance_summary": self.get_performance_summary()
            }
            return json.dumps(export_data, indent=2)
        else:
            raise ValueError(f"Unsupported export format: {format}")


# Global monitor instance
_indexing_monitor: Optional[IndexingMonitor] = None


async def get_indexing_monitor(config: MonitoringConfiguration = None) -> IndexingMonitor:
    """Get or create the global indexing monitor instance."""
    global _indexing_monitor
    
    if _indexing_monitor is None:
        _indexing_monitor = IndexingMonitor(config)
        await _indexing_monitor.start_monitoring()
    
    return _indexing_monitor


async def log_indexing_event(event_type: str, **kwargs):
    """Log an indexing event to the monitor."""
    monitor = await get_indexing_monitor()
    
    if event_type == "request_started":
        monitor.increment_counter("total_requests")
    elif event_type == "request_completed":
        monitor.increment_counter("completed_requests")
    elif event_type == "request_failed":
        monitor.increment_counter("failed_requests")
    elif event_type == "queue_size_update":
        monitor.set_gauge("queue_size", kwargs.get("size", 0))
    elif event_type == "latency_update":
        monitor.set_gauge("average_latency", kwargs.get("latency", 0.0))
    
    # Update any additional metrics passed in kwargs
    monitor.update_metrics(**kwargs)
